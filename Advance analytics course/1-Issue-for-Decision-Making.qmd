# Issue for Decision-Making {.numbered}

# Exploratory Analysis of Multidimensional Data {.unnumbered}

## Import Dataset, Load Required Packages, and Provide a Summary

```{r}
#| echo: true
#| message: false
#| warning: false
# working directory
#setwd(dirname(rstudioapi::getSourceEditorContext()$path))

# packages
list_packages = c('readxl', 'dplyr', 'moments', 'tidyr', 'tibble', 'gt', 'ggplot2', 'fmsb', 'car')
new.packages = list_packages[!(list_packages %in% installed.packages()[,"Package"])]
if (length(new.packages)) {
  install.packages(new.packages)
}
for (package in list_packages){
  library(package, character.only = T)
}

# Load the dataset
delitos_data <- read_excel("data/delitos_2024_v2.xlsx", 
    sheet = "2024")

dim(delitos_data)
summary(delitos_data)
```

### Augmented Data Analyst {.unnumbered}

### Prompts {.unnumbered}

## Skewness

Skewness measures the asymmetry of a data distribution around its mean. It is defined mathematically as:

$g_1 = \frac{n}{(n-1)(n-2)} \sum \left( \frac{x_i - \bar{x}}{\sigma} \right)^3$

where:

-   $( n )$: Number of observations,
-   $( x_i )$: Individual data points,
-   $( \bar{x} )$: Mean of the data,
-   $( \sigma )$: Standard deviation of the data.

A skewness of $( 0 )$ indicates a perfectly symmetric distribution. Positive skewness $( g_1 > 0 )$ signifies a longer tail on the right side of the distribution, while negative skewness $( g_1 < 0 )$ indicates a longer tail on the left. The code below calculates skewness for all numeric columns in `delitos_data` and presents the results in a formatted table:

```{r}
#| echo: true
#| message: false
#| warning: false

# skewness
delitos_data %>%
  select(where(is.numeric)) %>%  # Select only numeric columns
  summarise(across(everything(), ~ skewness(.x, na.rm = TRUE))) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(Skewness = V1) %>%
  gt()
```

### Augmented Data Analyst {.unnumbered}

### Prompts {.unnumbered}

## Kurtosis

Kurtosis measures the heaviness of the tails of a data distribution relative to a normal distribution. It is defined mathematically as:

$g_2 = \left[ \frac{n(n+1)}{(n-1)(n-2)(n-3)} \sum \left( \frac{x_i - \bar{x}}{\sigma} \right)^4 \right] - \frac{3(n-1)^2}{(n-2)(n-3)}$

where:

-   $( n )$: Number of observations,
-   $( x_i )$: Individual data points,
-   $( \bar{x} )$: Mean of the data,
-   $( \sigma )$: Standard deviation of the data.

A kurtosis of $( 0 )$ (excess kurtosis) indicates tail behavior similar to a normal distribution. Positive kurtosis $( g_2 > 0 )$ signifies heavier tails and more outliers (leptokurtic), while negative kurtosis $( g_2 < 0 )$ indicates lighter tails and fewer outliers (platykurtic).

```{r}
#| echo: true
#| message: false
#| warning: false

# kurtosis
delitos_data %>%
  select(where(is.numeric)) %>%  # Select only numeric columns
  summarise(across(everything(), ~ kurtosis(.x, na.rm = TRUE))) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(Kurtosis = V1) %>%
  gt()
```

### Augmented Data Analyst {.unnumbered}

### Prompts {.unnumbered}

## Coefficient of Variation

The coefficient of variation (CV) measures the relative variability of a dataset, expressed as a percentage. It is defined mathematically as:

$CV = \left( \frac{\sigma}{\bar{x}} \right) \times 100$

where:

-   $( \sigma )$: Standard deviation of the data,
-   $( \bar{x} )$: Mean of the data.

The coefficient of variation is particularly useful for comparing the variability of datasets with different units or widely different means. A lower CV indicates less variability relative to the mean, while a higher CV indicates greater variability.

```{r}
#| echo: true
#| message: false
#| warning: false

# Standard Deviation (SD)
delitos_data %>%
  select(where(is.numeric)) %>%
  summarise(across(everything(), ~ sd(.x, na.rm = TRUE))) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(SD = V1) %>%
  gt()

# variation
delitos_data %>%
  select(where(is.numeric)) %>%  # Select only numeric columns
  summarise(
    across(
      everything(),
      ~ ifelse(mean(.x, na.rm = TRUE) != 0, 
               sd(.x, na.rm = TRUE) / mean(.x, na.rm = TRUE), 
               NA),  # Compute CV safely
      .names = "{col}"
    )
  ) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(Variation = V1) %>%
  gt()
```

### Augmented Data Analyst {.unnumbered}

### Prompts {.unnumbered}

## Median Absolute Deviation MAD and MAD/median

The Median Absolute Deviation (MAD) is a robust measure of variability that quantifies the dispersion of a dataset. It is defined as the median of the absolute deviations from the median of the data:

$\text{MAD} = \text{median} \left( \left| x_i - \text{median}(x) \right| \right)$

where:

-   ( x_i ): Individual data points,
-   ( \text{median}(x) ): Median of the data.

The MAD/Median ratio is a normalized measure of dispersion, calculated as:

$[
\text{MAD/Median} = \frac{\text{MAD}}{\text{median}(x)}
]$

This ratio provides a scale-independent measure of variability, making it useful for comparing the dispersion of datasets with different units or scales. A higher MAD/Median ratio indicates greater relative variability.

```{r}
#| echo: true
#| message: false
#| warning: false

# MAD
delitos_data %>%
  select(where(is.numeric)) %>%
  summarise(across(everything(), ~ mad(.x, na.rm = TRUE))) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(MAD = V1) %>%
  gt()

# MAD/Median
delitos_data %>%
  select(where(is.numeric)) %>%
  summarise(across(everything(), ~ ifelse(
    median(.x, na.rm = TRUE) != 0,
    mad(.x, na.rm = TRUE) / median(.x, na.rm = TRUE),
    NA
  ))) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(`MAD/Median` = V1) %>%
  gt()

delitos_data %>%
  select(SEMOVIENTES, AUTOMOTORES, BANCOS, PIRATERIA) %>%
  summary()
```

### Augmented Data Analyst {.unnumbered}

### Prompts {.unnumbered}

## Covariance Matrix

The covariance matrix $( \Sigma )$ captures the pairwise covariances between variables in a dataset. For a dataset $( X )$ with $( n )$ observations and $( p )$ variables, the covariance matrix is defined as:

$\Sigma = \frac{1}{n-1} (X - \bar{X})^\top (X - \bar{X})$

where:

-   $( X )$ is the $( n \times p )$ data matrix.
-   $( \bar{X} )$ is the $( n \times p )$ matrix of column means.
-   $( \Sigma )$ is a $( p \times p )$ symmetric matrix.

```{r}
#| echo: true
#| message: false
#| warning: false
delitos_data %>%
  select(where(is.numeric)) %>%
  cov() %>%
  round(2) %>%
  knitr::kable(digits = 2, caption = "Covariance Matrix")
```

### Augmented Data Analyst {.unnumbered}

### Prompts {.unnumbered}

## Covariance Matrix of Log-Transformed Data

To handle skewed data or reduce the impact of outliers, we apply a log transformation to the data. Let $( Y = \log(X + 1) )$, where $( \log )$ is applied element-wise and $( 1 )$ is a matrix of ones to handle zeros. The log-transformed covariance matrix $( \Sigma_{\text{log}} )$ is:

$\Sigma_{\text{log}} = \frac{1}{n-1} (Y - \bar{Y})^\top (Y - \bar{Y})$

where:

-   $( Y )$ is the $( n \times p )$ log-transformed data matrix.
-   $( \bar{Y} )$ is the $( n \times p )$ matrix of column means of $( Y )$.

```{r}
#| echo: true
#| message: false
#| warning: false

delitos_data %>%
  select(where(is.numeric)) %>%
  mutate(across(everything(), ~ log(.x+1))) %>%  # Log-transform (+1 to handle zeros)
  cov() %>%
  round(2) %>%
  knitr::kable(digits = 2, caption = "Covariance Matrix (Log-Transformed)")
```

### Augmented Data Analyst {.unnumbered}

### Prompts {.unnumbered}

## Redundant Variables  

Redundant variables provide little additional information due to high correlation with others, leading to multicollinearity in models.  

Mathematically, redundancy is detected using the covariance matrix $\Sigma$, whose eigenvalues $\lambda_i$ and eigenvectors $v_i$ capture variance directions. A small eigenvalue $\lambda_{\min} \approx 0$ suggests a near-linear dependency:  

$\Sigma v_{\min} = \lambda_{\min} v_{\min}$

The eigenvector $v_{\\min}$ identifies the redundant variable combination.  

To confirm, we fit a regression model where one variable $y$ is explained by others:  

$y = \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon$

To quantify redundancy, we compute Variance Inflation Factors (VIFs) for each predictor $x_j$:  

$VIF_j = \frac{1}{1 - R_j^2}$

where $R_j^2$ is the $R^2$ value from regressing $x_j$ on all other predictors.  

- $VIF_j = 1$ → No multicollinearity.  
- $VIF_j > 5$ → Moderate multicollinearity.  
- $VIF_j > 10$ → Severe multicollinearity, indicating redundancy.  

A high VIF suggests that $x_j$ contributes little independent information and may be removed to improve model stability.

```{r}
#| echo: true
#| message: false
#| warning: false

# Covariance matrix 
cm_delitos_data <- delitos_data %>%
  select(where(is.numeric)) %>%
  cov()

# Compute eigenvalues and eigenvectors
eigen_results <- cm_delitos_data %>% eigen()

# Extract eigenvalues and eigenvectors
eigenvalues <- eigen_results$values
eigenvectors <- eigen_results$vectors

# Display eigenvalues and eigenvectors
print(eigenvalues)
print(eigenvectors)

# The Smallest Eigenvalues
sort(eigenvalues, decreasing = FALSE)[1:5]

# The smallest eigenvalue is approximately zero
smallest_eigenvalue <- min(eigenvalues)
print(smallest_eigenvalue)

# Corresponding eigenvector
smallest_eigenvector <- eigenvectors[, which.min(eigenvalues)]
print(smallest_eigenvector)

# Normalize the eigenvector by dividing by the largest absolute value
normalized_eigenvector <- smallest_eigenvector / max(abs(smallest_eigenvector))
print(normalized_eigenvector)

# Sorted normalize the eigenvector
sort(abs(normalized_eigenvector), decreasing = T)

# Get numeric variable names (order matches eigenvector indices)
variable_names <- colnames(delitos_data %>% select(where(is.numeric)))

# Sort normalized eigenvector by absolute contribution (descending order)
sorted_contributions <- sort(abs(normalized_eigenvector), decreasing = TRUE)

# Get the indices of the top contributions
top_indices <- order(abs(normalized_eigenvector), decreasing = TRUE)[1:4]

# Get the names of the top variables
top_variable_names <- variable_names[top_indices]

# Print the top variable names
print(top_variable_names)

# Fit a regression model to confirm the relationship
model <- lm(BANCOS ~ PIRATERIA + SECUESTRO + TERRORISMO, 
            data = data.frame(cm_delitos_data))
summary(model)

# Variance Inflation Factors
vif(model)
```

### Augmented Data Analyst {.unnumbered}

### Prompts {.unnumbered}

## Global Variability Metric

The effective variance and effective standard deviation are measures of the overall variability in the dataset. They are derived from the determinant of the covariance matrix, which captures the generalized variance of the data. For log-transformed data, these metrics are computed similarly but on the log-transformed covariance matrix.

The effective variance is defined as:

Effective Variance $= \det(\Sigma)^{\frac{1}{p}}$

where:

-   $( \Sigma )$ is the covariance matrix.
-   $( p )$ is the number of variables.

The effective standard deviation is given by:

-   Effective Standard Deviation $= \det(\Sigma)^{\frac{1}{2p}}$

For log-transformed data, the effective variance is computed as:

-   Log-Transformed Effective Variance $= \det(\log(\Sigma + 1))^{\frac{1}{p}}$

Similarly, the log-transformed effective standard deviation is:

-   Log-Transformed Effective Standard Deviation $= \det(\log(\Sigma + 1))^{\frac{1}{2p}}$

```{r}
#| echo: true
#| message: false
#| warning: false

cov_matrix <- delitos_data %>%
  select(-BANCOS) %>%
  select(where(is.numeric)) %>%  # Select numeric columns
  cov() 

# Effective Variance
det(cov_matrix)^(1/ncol(cov_matrix))

# Log-Transformed Effective Variance
det(log(cov_matrix + 1))^(1/ncol(cov_matrix))

# Effective Standard Deviation
det(cov_matrix)^(1/(ncol(cov_matrix) * 2))

# Log-Transformed Effective Standard Deviation
det(log(cov_matrix + 1))^(1/(ncol(cov_matrix) * 2))
```

### Augmented Data Analyst {.unnumbered}

### Prompts {.unnumbered}

## Linear Dependency and Precision Matrix

```{r}
#| echo: true
#| message: false
#| warning: false

# Compute precision matrix
S_inv <- solve(cov_matrix)

# Display precision matrix (should match example values)
cat("Precision Matrix (S⁻¹):\n")
print(S_inv, digits = 2)

# Extract first row components
first_row <- S_inv[1,]
diag_element <- S_inv[1,1]

# Calculate regression coefficients
beta_coefficients <- -first_row[-1]/diag_element
print(beta_coefficients, digits = 2)

# Calculate residual variance and standar error
residual_variance <- 1/diag_element
round(residual_variance, 2)
sqrt(round(residual_variance, 2))

# R^2
1 - (1/(cov_matrix[1,1] * S_inv[1,1]))

# Regression coefficients
delitos <- delitos_data %>%
  select(-BANCOS) %>%
  select(where(is.numeric))

model <- lm(SEXUALES ~ ., data = data.frame(delitos))
summary(model)
```

### Augmented Data Analyst {.unnumbered}

### Prompts {.unnumbered}

# Graphical analysis {.unnumbered}

## Distribution of Crime Data

```{r}
#| echo: true
#| message: false
#| warning: false

# Load necessary library
library(ggplot2)
library(tidyr)

# Transform the data to a long format for ggplot
delitos_long <- delitos_data %>%
  select(where(is.numeric)) %>%  # Select only numeric columns
  pivot_longer(cols = everything(), names_to = "Crime Type", values_to = "Value")

# Create faceted histograms
ggplot(delitos_long, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
  facet_wrap(~ `Crime Type`, scales = "free") +  # Facet by crime type
  theme_minimal() +
  labs(
    title = "Distributions of Crime Data",
    x = "Value",
    y = "Frequency"
  ) +
  theme(
    axis.text.x = element_text(size = 5)  # Reduce the font size of X-axis text
  )
```

## Log-Transformed Crime Data Distributions

```{r}
#| echo: true
#| message: false
#| warning: false

# Transform the data to long format and apply log transformation
delitos_long_log <- delitos_data %>%
  select(where(is.numeric)) %>%
  mutate(across(everything(), ~ log(.x), .names = "{col}")) %>%  # Log transform (log(x + 1) to avoid log(0))
  pivot_longer(cols = everything(), names_to = "Crime Type", values_to = "Log Value")

# Create faceted histograms for log-transformed values
ggplot(delitos_long_log, aes(x = `Log Value`)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
  facet_wrap(~ `Crime Type`, scales = "free") +  # Facet by crime type
  theme_minimal() +
  labs(
    title = "Log-Transformed Distributions of Crime Data",
    x = "Log Value",
    y = "Frequency"
  ) +
  theme(
    axis.text.x = element_text(size = 3)  # Reduce the font size of X-axis text
  )
```

# Hands-on Data Analysis {.unnumbered}

## Tackling a Critical Challenge: The Proliferation of Spatial Criminal Phenomena

### Mathematical Definitions:

-   Polygons: Represented as $P = \{ p_1, p_2, \dots, p_n \}$ where each polygon $( p_i )$ has geometric and attribute data.

-   Phenomenon: A discrete or continuous variable $( Y )$ observed across $( P )$.

-   Adjacency Matrix: $( A )$ where

    $A_{ij} = \begin{cases} 1, & \text{if } p_i \text{ and } p_j \text{ share a boundary} \\ 0, & \text{otherwise} \end{cases}$

-   Distance Matrix: $( D )$ where $( D_{ij} )$ represents the distance between the centroids of $( p_i )$ and $( p_j )$.

### Phenomenon Classification:

-   Local (Within Polygon): Phenomenon occurs exclusively inside a single polygon $( p_i )$. Thus, $Y_i = f(\text{Internal factors of } p_i)$.

-   Subnational (Cluster of Neighbors): Phenomenon clusters among adjacent polygons. Thus, $Y_i = \beta_0 + \beta_1 \sum_{j \in N(i)} Y_j + \epsilon$ where $( N(i) )$ are the neighbors of $( p_i )$.

-   National/International (Neighbors + Non-Neighbors): Phenomenon spans both adjacent and non-adjacent polygons (e.g., trade networks). Thus, $Y_i = f\left(\sum_j w_{ij} Y_j\right)$ where $( w_{ij} )$ depends on distance or network connectivity.

-   Local-Subnational (Clear Boundaries): Clear separation between local (city) and subnational (state) boundaries among neighbors. Thus, $Y_i = \alpha + \gamma \cdot \text{State}_i + \epsilon$.

-   Local-National/International (Non-Neighbors): Phenomenon connects non-neighboring polygons with clear boundaries (e.g., migration between distant cities). Thus, $Y_i = f(Y_j)$ where $( A_{ij} = 0 )$ (i.e., not adjacent) but $( D_{ij} )$ is significant.

-   Local-Subnational (Fuzzy Boundaries): Ambiguous administrative boundaries (e.g., overlapping jurisdictions). Thus, $\mu_{ik} \in [0,1]$ indicating the degree to which $( p_i )$ belongs to region $( k )$.

## Final Project Activities

-   Select stations that align with your analysis interests to illustrate spatial criminal phenomena.
-   Replicate the exploratory analysis performed on the selected stations as demonstrated in the lecture.
-   Define the issue for decision-making that you propose.
-   Upload your work by creating a GitHub pull request with your group's .qmd file.
