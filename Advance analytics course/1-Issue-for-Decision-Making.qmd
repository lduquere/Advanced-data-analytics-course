# Issue for Decision-Making {.numbered}

This section is based on *Análisis de datos multivariantes*. See [@peña2002análisis].

# Exploratory Analysis of Multidimensional Data {.unnumbered}

## Import Dataset, Load Required Packages, and Provide a Summary

```{r}
#| echo: true
#| message: false
#| warning: false
# working directory
#setwd(dirname(rstudioapi::getSourceEditorContext()$path))

# packages
list_packages = c('readxl', 'dplyr', 'moments', 'tidyr', 'tibble', 'gt', 'ggplot2', 'fmsb', 'car', 'reshape2', 'knitr', 'gridExtra', 'ggExtra', 'sf')
new.packages = list_packages[!(list_packages %in% installed.packages()[,"Package"])]
if (length(new.packages)) {
  install.packages(new.packages)
}
for (package in list_packages){
  library(package, character.only = T)
}

# Load the dataset
delitos_data <- st_read("data/spatial/crime_spatial_course.gpkg")
delitos_data <- delitos_data[delitos_data$dpto_ccdgo == '11', ]

dim(delitos_data)
summary(delitos_data)
plot(delitos_data$geo)

# quantile
quantile(delitos_data$sum_24HP, probs = seq(0, 1, 0.1), na.rm = TRUE)

#boxplot
boxplot(delitos_data$sum_24HP, main = "Boxplot of PERSONAS", horizontal = TRUE)
```

## Skewness

Skewness measures the asymmetry of a data distribution around its mean. It is defined mathematically as:

$g_1 = \frac{n}{(n-1)(n-2)} \sum \left( \frac{x_i - \bar{x}}{\sigma} \right)^3$

where:

-   $( n )$: Number of observations,
-   $( x_i )$: Individual data points,
-   $( \bar{x} )$: Mean of the data,
-   $( \sigma )$: Standard deviation of the data.

A skewness of $( 0 )$ indicates a perfectly symmetric distribution. Positive skewness $( g_1 > 0 )$ signifies a longer tail on the right side of the distribution, while negative skewness $( g_1 < 0 )$ indicates a longer tail on the left. The code below calculates skewness for all numeric columns in `delitos_data` and presents the results in a formatted table:

```{r}
#| echo: true
#| message: false
#| warning: false

# step by step
n <- length(delitos_data$sum_24HP) 
mean_x <- mean(delitos_data$sum_24HP)
sd_x <- sd(delitos_data$sum_24HP)  # Uses (n-1) denominator
z_scores <- (delitos_data$sum_24HP - mean_x) / sd_x
z_cubed <- z_scores^3
sum_cubed <- sum(z_cubed)
skewness <- (n / ((n - 1) * (n - 2))) * sum_cubed
paste0('sum_24HP: ', skewness)

# function
skewness(delitos_data$sum_24HP, na.rm = TRUE)

# skewness
delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  summarise(across(everything(), ~ skewness(.x, na.rm = TRUE))) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(Skewness = V1) %>%
  gt()
```

## Kurtosis

Kurtosis measures the heaviness of the tails of a data distribution relative to a normal distribution. It is defined mathematically as:

$g_2 = \left[ \frac{n(n+1)}{(n-1)(n-2)(n-3)} \sum \left( \frac{x_i - \bar{x}}{\sigma} \right)^4 \right] - \frac{3(n-1)^2}{(n-2)(n-3)}$

where:

-   $( n )$: Number of observations,
-   $( x_i )$: Individual data points,
-   $( \bar{x} )$: Mean of the data,
-   $( \sigma )$: Standard deviation of the data.

A kurtosis of $( 0 )$ (excess kurtosis) indicates tail behavior similar to a normal distribution. Positive kurtosis $( g_2 > 0 )$ signifies heavier tails and more outliers (leptokurtic), while negative kurtosis $( g_2 < 0 )$ indicates lighter tails and fewer outliers (platykurtic).

```{r}
#| echo: true
#| message: false
#| warning: false

# step by step
z_fourth <- z_scores^4
sum_fourth <- sum(z_fourth)
kurtosis <- ((n * (n + 1)) / ((n - 1) * (n - 2) * (n - 3))) * sum_fourth - (3 * (n - 1)^2) / ((n - 2) * (n - 3))
print(kurtosis)

# function
kurtosis(delitos_data$sum_24HP, na.rm = TRUE)

# skewness
delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  summarise(across(everything(), ~ kurtosis(.x, na.rm = TRUE))) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(Skewness = V1) %>%
  gt()
```

## Coefficient of Variation

The coefficient of variation (CV) measures the relative variability of a dataset, expressed as a percentage. It is defined mathematically as:

$CV = \left( \frac{\sigma}{\bar{x}} \right) \times 100$

where:

-   $( \sigma )$: Standard deviation of the data,
-   $( \bar{x} )$: Mean of the data.

The coefficient of variation is particularly useful for comparing the variability of datasets with different units or widely different means. A lower CV indicates less variability relative to the mean, while a higher CV indicates greater variability.

```{r}
#| echo: true
#| message: false
#| warning: false

# Compute statistics
mean_val <- mean(delitos_data$sum_24HP, na.rm = TRUE)
print(mean_val)
std_dev <- sd(delitos_data$sum_24HP, na.rm = TRUE)
print(std_dev)

# Compute the range for first standard deviation
lower_bound <- mean_val - std_dev
upper_bound <- mean_val + std_dev
paste0('lower_bound: ', round(lower_bound, 2), ' - upper_bound: ', round(upper_bound, 2))

# Count the number of points within 1 standard deviation
within_1sd <- sum(delitos_data$sum_24HP >= lower_bound & delitos_data$sum_24HP <= upper_bound, na.rm = TRUE)
percentage_1sd <- (within_1sd / nrow(delitos_data)) * 100
paste0('within_1sd: ', round(within_1sd, 2), ' - percentage_1sd: ', round(percentage_1sd, 2))

# Create histogram
ggplot(delitos_data, aes(x = sum_24HP)) +
  geom_histogram(binwidth = 5, fill = "blue", alpha = 0.5, color = "black") +
  
  # Add vertical lines for mean, median, and 1st SD
  geom_vline(aes(xintercept = mean_val), color = "red", linetype = "dashed", size = 1.2) +
  #geom_vline(aes(xintercept = median_val), color = "green", linetype = "dashed", size = 1.2) +
  geom_vline(aes(xintercept = lower_bound), color = "purple", linetype = "dashed", size = 1) +
  geom_vline(aes(xintercept = upper_bound), color = "purple", linetype = "dashed", size = 1) +
  
  # Labels and title
  labs(title = "Histogram of AUTOMOTORES with Mean, and 1SD Range",
       x = "AUTOMOTORES Values", y = "Frequency") +
  
  # Add annotation for 1SD range
  annotate("text", x = mean_val, y = 10, 
           label = paste(round(percentage_1sd, 2), "% 1SD", sep = ""), 
           color = "black", size = 5, hjust = 0.5, vjust = -1) +
  
  theme_minimal()

# cv
paste0('cv: ', round(std_dev / mean_val * 100), 2)

# variation
delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  summarise(
    across(
      everything(),
      ~ ifelse(mean(.x, na.rm = TRUE) != 0, 
               sd(.x, na.rm = TRUE) / mean(.x, na.rm = TRUE), 
               NA),  # Compute CV safely
      .names = "{col}"
    )
  ) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(Variation = V1) %>%
  gt()
```

## Median Absolute Deviation MAD and MAD/median

The Median Absolute Deviation (MAD) is a robust measure of variability that quantifies the dispersion of a dataset. It is defined as the median of the absolute deviations from the median of the data:

$\text{MAD} = \text{median} \left( \left| x_i - \text{median}(x) \right| \right)$

where:

-   ( x_i ): Individual data points,
-   ( \text{median}(x) ): Median of the data.

The MAD/Median ratio is a normalized measure of dispersion, calculated as:

$[
\text{MAD/Median} = \frac{\text{MAD}}{\text{median}(x)}
]$

This ratio provides a scale-independent measure of variability, making it useful for comparing the dispersion of datasets with different units or scales. A higher MAD/Median ratio indicates greater relative variability.

```{r}
#| echo: true
#| message: false
#| warning: false

# Compute statistics
median_val <- median(delitos_data$sum_24HP, na.rm = TRUE)
print(median_val)
mad_val <- mad(delitos_data$sum_24HP, na.rm = TRUE)  # Compute MAD
print(mad_val)

# Compute the range for first standard deviation
lower_bound <- median_val - mad_val
upper_bound <- median_val + mad_val
paste0('lower_bound: ', round(lower_bound, 2), ' - upper_bound: ', round(upper_bound, 2))

# Count the number of points within 1 MAD
within_1mad <- sum(delitos_data$sum_24HP >= lower_bound & delitos_data$sum_24HP <= upper_bound, na.rm = TRUE)
percentage_1mad <- (within_1mad / nrow(delitos_data)) * 100
paste0('within_1mad: ', round(within_1mad, 2), ' - percentage_1mad: ', round(percentage_1mad, 2))

# Create histogram
ggplot(delitos_data, aes(x = sum_24HP)) +
  geom_histogram(binwidth = 5, fill = "blue", alpha = 0.5, color = "black") +
  
  # Add vertical lines for mean, median, and 1st SD
  #geom_vline(aes(xintercept = mean_val), color = "red", linetype = "dashed", size = 1.2) +
  geom_vline(aes(xintercept = median_val), color = "green", linetype = "dashed", size = 1.2) +
  geom_vline(aes(xintercept = lower_bound), color = "purple", linetype = "dashed", size = 1) +
  geom_vline(aes(xintercept = upper_bound), color = "purple", linetype = "dashed", size = 1) +
  
  # Labels and title
  labs(title = "Histogram of AUTOMOTORES with Median, and 1MAD Range",
       x = "AUTOMOTORES Values", y = "Frequency") +
  
  # Add annotation for 1SD range
  annotate("text", x = median_val, y = 10, 
           label = paste(within_1mad, "points (", round(percentage_1mad, 2), "%) 1MAD", sep = ""), 
           color = "black", size = 5, hjust = 0.5, vjust = -1) +
  
  theme_minimal()

# MAD/Median
paste0('MAD/Median: ', round(mad_val / median_val * 100), 2)
```

## Covariance Matrix

The covariance matrix $( \Sigma )$ captures the pairwise covariances between variables in a dataset. For a dataset $( X )$ with $( n )$ observations and $( p )$ variables, the covariance matrix is defined as:

$\Sigma = \frac{1}{n-1} (X - \bar{X})^\top (X - \bar{X})$

where:

-   $( X )$ is the $( n \times p )$ data matrix.
-   $( \bar{X} )$ is the $( n \times p )$ matrix of column means.
-   $( \Sigma )$ is a $( p \times p )$ symmetric matrix.

```{r}
#| echo: true
#| message: false
#| warning: false
delitos_data %>%
  st_drop_geometry() %>%
  select(contains("24")) %>%
  cov() %>%
  round(2) %>%
  knitr::kable(digits = 2, caption = "Covariance Matrix")
```

## Covariance Matrix of Log-Transformed Data

To handle skewed data or reduce the impact of outliers, we apply a log transformation to the data. Let $( Y = \log(X + 1) )$, where $( \log )$ is applied element-wise and $( 1 )$ is a matrix of ones to handle zeros. The log-transformed covariance matrix $( \Sigma_{\text{log}} )$ is:

$\Sigma_{\text{log}} = \frac{1}{n-1} (Y - \bar{Y})^\top (Y - \bar{Y})$

where:

-   $( Y )$ is the $( n \times p )$ log-transformed data matrix.
-   $( \bar{Y} )$ is the $( n \times p )$ matrix of column means of $( Y )$.

We are going to begin by understanding log transformation, a key tool for handling multiplicative relationships in data.

-   Compresses large values to reduce skewness.
-   Converts multiplicative relationships into additive ones.
-   Eases interpretation when values span multiple orders of magnitude.

```{r}
#| echo: true
#| message: false
#| warning: false

# Define the dataset
x <- delitos_data$sum_24HP

# 1. Compute Raw Arithmetic Mean
arithmetic_mean <- mean(x)
print(arithmetic_mean)

# 2. Compute Log-Mean (Multiplicative Center)
log_x <- log(x + 1)  # Take logarithm of values
head(log_x)
log_mean <- mean(log_x)  # Compute mean in log-space
print(log_mean)
log_mean_exp <- exp(log_mean)  # Convert back to original scale
print(log_mean_exp)

# Create the comparison table
comparison_table <- data.frame(
  Index = seq_along(x),  # Just an index for x-axis
  Original_Value = x,
  Log_Value = log_x
)

p1 <- ggplot(comparison_table, aes(x = Original_Value, y = Log_Value)) +
  geom_line(color = "gray70", size = 0.7, alpha = 0.5) +  # Thin line connecting points
  geom_point(alpha = 0.7, color = "blue") +  # Scatter points with transparency
  labs(
    title = "Scatter Plot: Original vs. Log-Transformed Values",
    x = "Original Values",
    y = "Log-Transformed Values"
  ) +
  theme_minimal()

# Add marginal histogram
ggMarginal(
  p1,
  type = "histogram",         # Add marginal histograms
  bins = 40,                  # Number of bins for the histogram
  margins = "both",           # Add histogram to both x and y margins
  size = 5,                   # Size of the histograms relative to the scatter plot
  fill = "gray",              # Fill color for the histogram
  color = "black",            # Outline color for the histogram
  alpha = 0.5                 # Transparency
)
```

```{r}
#| include: false
# Store values for inline Quarto text
log_values <- paste(round(head(comparison_table$Log_Value), 2), collapse = ", ")
original_values <- paste(head(comparison_table$Original_Value), collapse = ", ")
```

Euler steps describe how many multiplicative steps of $( e )$ are needed to reach a given value.

For example, in our dataset:

-   Original Values: `r original_values`\
-   Log Values: `r log_values`

Each log-transformed value represents the number of times we need to multiply 1 by $( e )$ to reach the original value:

$e^ \text{Log Values} = \text{Original Value}$

```{r}
#| echo: true
#| message: false
#| warning: false

#log transformed data
# Compute statistics for raw and log-transformed data
mean_raw <- mean(delitos_data$sum_24HP, na.rm = TRUE)
sd_raw <- sd(delitos_data$sum_24HP, na.rm = TRUE)
mad_raw <- mad(delitos_data$sum_24HP, na.rm = TRUE)

delitos_data_log <- delitos_data %>%
  #mutate(LOG_AUTOMOTORES = log(AUTOMOTORES + 1))
  mutate(LOG_AUTOMOTORES = log1p(sum_24HP))  # log1p(x) = log(1 + x) to handle zeros

mean_log <- mean(delitos_data_log$LOG_AUTOMOTORES, na.rm = TRUE)
sd_log <- sd(delitos_data_log$LOG_AUTOMOTORES, na.rm = TRUE)
mad_log <- mad(delitos_data_log$LOG_AUTOMOTORES, na.rm = TRUE)

# Compute statistics for raw and log-transformed data
data.frame(
  Measure = c("Mean", "Median", "Standard Deviation", "MAD"),
  Raw_Data = c(mean(delitos_data$sum_24HP, na.rm = TRUE),
               median(delitos_data$sum_24HP, na.rm = TRUE),
               sd(delitos_data$sum_24HP, na.rm = TRUE),
               mad(delitos_data$sum_24HP, na.rm = TRUE)),
  Log_Transformed_Data = c(mean(delitos_data_log$LOG_AUTOMOTORES, na.rm = TRUE),
                           median(delitos_data_log$LOG_AUTOMOTORES, na.rm = TRUE),
                           sd(delitos_data_log$LOG_AUTOMOTORES, na.rm = TRUE),
                           mad(delitos_data_log$LOG_AUTOMOTORES, na.rm = TRUE)))

# Transform the data to a long format for ggplot
delitos_long <- delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  pivot_longer(cols = everything(), names_to = "Crime Type", values_to = "Value")

# Create faceted histograms
ggplot(delitos_long, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
  facet_wrap(~ `Crime Type`, scales = "free") +  # Facet by crime type
  theme_minimal() +
  labs(
    title = "Distributions of Crime Data",
    x = "Value",
    y = "Frequency"
  ) +
  theme(
    axis.text.x = element_text(size = 5)  # Reduce the font size of X-axis text
  )

# Transform the data to long format and apply log transformation
delitos_long_log <- delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  mutate(across(everything(), ~ log(.x), .names = "{col}")) %>%  # Log transform (log(x + 1) to avoid log(0))
  pivot_longer(cols = everything(), names_to = "Crime Type", values_to = "Log Value")

# Create faceted histograms for log-transformed values
ggplot(delitos_long_log, aes(x = `Log Value`)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
  facet_wrap(~ `Crime Type`, scales = "free") +  # Facet by crime type
  theme_minimal() +
  labs(
    title = "Log-Transformed Distributions of Crime Data",
    x = "Log Value",
    y = "Frequency"
  ) +
  theme(
    axis.text.x = element_text(size = 3)  # Reduce the font size of X-axis text
  )

# Covariance Matrix (Log-Transformed)
delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  mutate(across(everything(), ~ log(.x+1))) %>%  # Log-transform (+1 to handle zeros)
  cov() %>%
  round(2) %>%
  kable(digits = 2, caption = "Covariance Matrix (Log-Transformed)")
```

## Redundant Variables

Redundant variables provide little additional information due to high correlation with others, leading to multicollinearity in models.

Mathematically, redundancy is detected using the covariance matrix $\Sigma$, whose eigenvalues $\lambda_i$ and eigenvectors $v_i$ capture variance directions. A small eigenvalue $\lambda_{\min} \approx 0$ suggests a near-linear dependency:

$\Sigma v_{\min} = \lambda_{\min} v_{\min}$

The eigenvector $v_{\\min}$ identifies the redundant variable combination.

To confirm, we fit a regression model where one variable $y$ is explained by others:

$y = \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon$

To quantify redundancy, we compute Variance Inflation Factors (VIFs) for each predictor $x_j$:

$VIF_j = \frac{1}{1 - R_j^2}$

where $R_j^2$ is the $R^2$ value from regressing $x_j$ on all other predictors.

-   $VIF_j = 1$ → No multicollinearity.\
-   $VIF_j > 5$ → Moderate multicollinearity.\
-   $VIF_j > 10$ → Severe multicollinearity, indicating redundancy.

A high VIF suggests that $x_j$ contributes little independent information and may be removed to improve model stability.

Eigenvalues and eigenvectors are fundamental tools in linear algebra, representing the directions and scaling factors of a matrix transformation.

1.  Eigenvalues: Solving the Characteristic Equation

The eigenvalues $( \lambda )$ of a matrix $( A )$ satisfy:

$\det(A - \lambda I) = 0$

Where: - $( A )$ is the matrix. - $( \lambda )$ is the eigenvalue (unknown). - $( I )$ is the identity matrix (a diagonal matrix with 1s on the diagonal).

The characteristic polynomial is derived by computing $( \det(A - \lambda I) )$ and solving for $( \lambda )$.

Eigenvectors: Solving for Principal Directions

For each eigenvalue $( \lambda )$, the eigenvector $( v )$ satisfies:

$(A - \lambda I)v = 0$

This is a homogeneous system of linear equations. Solving this system gives the eigenvector(s) associated with each eigenvalue.

```{r}
#| echo: true
#| message: false
#| warning: false

# Define the matrix A
matrix_a <- matrix(c(4, 2,
                     2, 3), nrow = 2, byrow = TRUE)
print("Matrix A:")
print(matrix_a)

# Compute the eigen decomposition using R's built-in eigen() function
eigen_builtin <- eigen(matrix_a)
print("Built-in eigen() values:")
print(eigen_builtin$values)
print("Built-in eigen() vectors:")
print(eigen_builtin$vectors)

# Multiply A by the matrix of eigenvectors:
# Each column of eigen_builtin$vectors is an eigenvector.
res <- matrix_a %*% eigen_builtin$vectors
print("A * eigenvectors:")
print(res)

# Multiply the eigenvector matrix by the diagonal matrix of eigenvalues.
res2 <- eigen_builtin$vectors %*% diag(eigen_builtin$values)
print("eigenvectors * eigenvalues:")
print(res2)

# Check if these two matrices are equal (they should be equal within numerical precision)
are_equal <- all.equal(res, res2)
print("Are A * eigenvectors and eigenvectors * eigenvalues equal?")
print(are_equal)
```

```{r}
#| echo: true
#| message: false
#| warning: false

# Covariance matrix 
cm_delitos_data <- delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  cov()

# Compute eigenvalues and eigenvectors
eigen_results <- cm_delitos_data %>% eigen()

# Extract eigenvalues and eigenvectors
eigenvalues <- eigen_results$values
eigenvectors <- eigen_results$vectors

# Display eigenvalues and eigenvectors
print(eigenvalues)
head(eigenvectors)

# The Smallest Eigenvalues
sort(eigenvalues, decreasing = FALSE)[1:5]

# The smallest eigenvalue is approximately zero
smallest_eigenvalue <- min(eigenvalues)
print(smallest_eigenvalue)

# Corresponding eigenvector
smallest_eigenvector <- eigenvectors[, which.min(eigenvalues)]
print(smallest_eigenvector)

# Normalize the eigenvector by dividing by the largest absolute value
normalized_eigenvector <- smallest_eigenvector / max(abs(smallest_eigenvector))
print(normalized_eigenvector)

# Sorted normalize the eigenvector
sort(abs(normalized_eigenvector), decreasing = T)

# Get numeric variable names (order matches eigenvector indices)
variable_names <- colnames(delitos_data %>% select(where(is.numeric)))

# Sort normalized eigenvector by absolute contribution (descending order)
sorted_contributions <- sort(abs(normalized_eigenvector), decreasing = TRUE)

# Get the indices of the top contributions
top_indices <- order(abs(normalized_eigenvector), decreasing = TRUE)[1:4]

# Get the names of the top variables
top_variable_names <- variable_names[top_indices]

# Print the top variable names
print(top_variable_names)

# Fit a regression model to confirm the relationship
model <- lm(BANCOS ~ PIRATERIA + SECUESTRO + TERRORISMO, 
            data = data.frame(cm_delitos_data))
summary(model)

# Variance Inflation Factors
vif(model)
```

## Global Variability Metric

The effective variance and effective standard deviation are measures of the overall variability in the dataset. They are derived from the determinant of the covariance matrix, which captures the generalized variance of the data. For log-transformed data, these metrics are computed similarly but on the log-transformed covariance matrix.

The effective variance is defined as:

Effective Variance $= \det(\Sigma)^{\frac{1}{p}}$

where:

-   $( \Sigma )$ is the covariance matrix.
-   $( p )$ is the number of variables.

The effective standard deviation is given by:

-   Effective Standard Deviation $= \det(\Sigma)^{\frac{1}{2p}}$

For log-transformed data, the effective variance is computed as:

-   Log-Transformed Effective Variance $= \det(\log(\Sigma + 1))^{\frac{1}{p}}$

Similarly, the log-transformed effective standard deviation is:

-   Log-Transformed Effective Standard Deviation $= \det(\log(\Sigma + 1))^{\frac{1}{2p}}$

```{r}
#| echo: true
#| message: false
#| warning: false

cov_matrix <- delitos_data %>%
  select(-BANCOS) %>%
  select(where(is.numeric)) %>%  # Select numeric columns
  cov() 

# Effective Variance
det(cov_matrix)^(1/ncol(cov_matrix))

# Log-Transformed Effective Variance
det(log(cov_matrix + 1))^(1/ncol(cov_matrix))

# Effective Standard Deviation
det(cov_matrix)^(1/(ncol(cov_matrix) * 2))

# Log-Transformed Effective Standard Deviation
det(log(cov_matrix + 1))^(1/(ncol(cov_matrix) * 2))
```

## Linear Dependency and Precision Matrix

Linear dependency in data occurs when some variables can be expressed as linear combinations of others, leading to redundancy. This is identified through the covariance matrix $( \Sigma )$ and its eigenvalues, where a near-zero eigenvalue indicates dependency.

The precision matrix $( \Sigma^{-1} )$, the inverse of the covariance matrix, quantifies conditional dependencies. It highlights direct variable relationships, with zero entries indicating independence given other variables. These concepts are crucial for multicollinearity detection and improving model interpretability.

Multicollinearity occurs when predictor variables are highly correlated, making it difficult to isolate their individual effects in a model.

```{r}
#| echo: true
#| message: false
#| warning: false

# Compute precision matrix
S_inv <- solve(cov_matrix)

# Display precision matrix (should match example values)
cat("Precision Matrix (S⁻¹):\n")
print(S_inv, digits = 2)

# Extract first row components
first_row <- S_inv[1,]
diag_element <- S_inv[1,1]

# Calculate regression coefficients
beta_coefficients <- -first_row[-1]/diag_element
print(beta_coefficients, digits = 2)

# Calculate residual variance and standar error
residual_variance <- 1/diag_element
round(residual_variance, 2)
sqrt(round(residual_variance, 2))

# R^2
1 - (1/(cov_matrix[1,1] * S_inv[1,1]))

# Regression coefficients
delitos <- delitos_data %>%
  select(-BANCOS) %>%
  select(where(is.numeric))

model <- lm(SEXUALES ~ ., data = data.frame(delitos))
summary(model)
```

# Hands-on Data Analysis {.unnumbered}

## Tackling a Critical Challenge: The Proliferation of Spatial Criminal Phenomena

### Mathematical Definitions:

-   Polygons: Represented as $P = \{ p_1, p_2, \dots, p_n \}$ where each polygon $( p_i )$ has geometric and attribute data.

-   Phenomenon: A discrete or continuous variable $( Y )$ observed across $( P )$.

-   Adjacency Matrix: $( A )$ where

    $A_{ij} = \begin{cases} 1, & \text{if } p_i \text{ and } p_j \text{ share a boundary} \\ 0, & \text{otherwise} \end{cases}$

-   Distance Matrix: $( D )$ where $( D_{ij} )$ represents the distance between the centroids of $( p_i )$ and $( p_j )$.

### Phenomenon Classification:

-   Local (Within Polygon): Phenomenon occurs exclusively inside a single polygon $( p_i )$. Thus, $Y_i = f(\text{Internal factors of } p_i)$.

-   Subnational (Cluster of Neighbors): Phenomenon clusters among adjacent polygons. Thus, $Y_i = \beta_0 + \beta_1 \sum_{j \in N(i)} Y_j + \epsilon$ where $( N(i) )$ are the neighbors of $( p_i )$.

-   National/International (Neighbors + Non-Neighbors): Phenomenon spans both adjacent and non-adjacent polygons (e.g., trade networks). Thus, $Y_i = f\left(\sum_j w_{ij} Y_j\right)$ where $( w_{ij} )$ depends on distance or network connectivity.

-   Local-Subnational (Clear Boundaries): Clear separation between local (city) and subnational (state) boundaries among neighbors. Thus, $Y_i = \alpha + \gamma \cdot \text{State}_i + \epsilon$.

-   Local-National/International (Non-Neighbors): Phenomenon connects non-neighboring polygons with clear boundaries (e.g., migration between distant cities). Thus, $Y_i = f(Y_j)$ where $( A_{ij} = 0 )$ (i.e., not adjacent) but $( D_{ij} )$ is significant.

-   Local-Subnational (Fuzzy Boundaries): Ambiguous administrative boundaries (e.g., overlapping jurisdictions). Thus, $\mu_{ik} \in [0,1]$ indicating the degree to which $( p_i )$ belongs to region $( k )$.

## Final Project Activities

-   Group stations based on your analysis interests to illustrate spatial criminal phenomena.
-   Define the issue for decision-making that you propose supported by the exploratory analysis of multidimensional data.
-   Upload your work by creating a GitHub pull request with your group's `.qmd` file in the appendix section.

## Class Participation Assessment

-   Replicate the exploratory analysis on the selected stations as demonstrated in the lecture, and compare your results with those of other groups.\
-   Upload your analysis under the respective subsections titled `Augmented Data Analyst` and `Prompts`.
