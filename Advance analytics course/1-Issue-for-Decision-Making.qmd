# Issue for Decision-Making {.numbered}

# Exploratory Analysis of Multidimensional Data {.unnumbered}

## Import Dataset, Load Required Packages, and Provide a Summary

```{r}
#| echo: true
#| message: false
#| warning: false
# working directory
#setwd(dirname(rstudioapi::getSourceEditorContext()$path))

# packages
list_packages = c('readxl', 'dplyr', 'moments', 'tidyr', 'tibble', 'gt', 'ggplot2', 'fmsb', 'car')
new.packages = list_packages[!(list_packages %in% installed.packages()[,"Package"])]
if (length(new.packages)) {
  install.packages(new.packages)
}
for (package in list_packages){
  library(package, character.only = T)
}

# Load the dataset
delitos_data <- read_excel("data/delitos_2024_v2.xlsx", 
    sheet = "2024")

dim(delitos_data)
summary(delitos_data)
```

## Skewness

Skewness measures the asymmetry of a data distribution around its mean. It is defined mathematically as:

$[
g_1 = \frac{n}{(n-1)(n-2)} \sum \left( \frac{x_i - \bar{x}}{\sigma} \right)^3
]$

where:

-   $( n )$: Number of observations,
-   $( x_i )$: Individual data points,
-   $( \bar{x} )$: Mean of the data,
-   $( \sigma )$: Standard deviation of the data.

A skewness of $( 0 )$ indicates a perfectly symmetric distribution. Positive skewness $( g_1 > 0 )$ signifies a longer tail on the right side of the distribution, while negative skewness $( g_1 < 0 )$ indicates a longer tail on the left. The code below calculates skewness for all numeric columns in `delitos_data` and presents the results in a formatted table:

```{r}
#| echo: true
#| message: false
#| warning: false

# skewness
delitos_data %>%
  select(where(is.numeric)) %>%  # Select only numeric columns
  summarise(across(everything(), ~ skewness(.x, na.rm = TRUE))) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(Skewness = V1) %>%
  gt()
```

## Kurtosis

Kurtosis measures the heaviness of the tails of a data distribution relative to a normal distribution. It is defined mathematically as:

$[
g_2 = \left[ \frac{n(n+1)}{(n-1)(n-2)(n-3)} \sum \left( \frac{x_i - \bar{x}}{\sigma} \right)^4 \right] - \frac{3(n-1)^2}{(n-2)(n-3)}
]$

where:

-   $( n )$: Number of observations,
-   $( x_i )$: Individual data points,
-   $( \bar{x} )$: Mean of the data,
-   $( \sigma )$: Standard deviation of the data.

A kurtosis of $( 0 )$ (excess kurtosis) indicates tail behavior similar to a normal distribution. Positive kurtosis $( g_2 > 0 )$ signifies heavier tails and more outliers (leptokurtic), while negative kurtosis $( g_2 < 0 )$ indicates lighter tails and fewer outliers (platykurtic).

```{r}
#| echo: true
#| message: false
#| warning: false

# kurtosis
delitos_data %>%
  select(where(is.numeric)) %>%  # Select only numeric columns
  summarise(across(everything(), ~ kurtosis(.x, na.rm = TRUE))) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(Kurtosis = V1) %>%
  gt()
```

## Coefficient of Variation

The coefficient of variation (CV) measures the relative variability of a dataset, expressed as a percentage. It is defined mathematically as:

$[
CV = \left( \frac{\sigma}{\bar{x}} \right) \times 100
]$

where:

-   $( \sigma )$: Standard deviation of the data,
-   $( \bar{x} )$: Mean of the data.

The coefficient of variation is particularly useful for comparing the variability of datasets with different units or widely different means. A lower CV indicates less variability relative to the mean, while a higher CV indicates greater variability.

```{r}
#| echo: true
#| message: false
#| warning: false

# Standard Deviation (SD)
delitos_data %>%
  select(where(is.numeric)) %>%
  summarise(across(everything(), ~ sd(.x, na.rm = TRUE))) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(SD = V1) %>%
  gt()

# variation
delitos_data %>%
  select(where(is.numeric)) %>%  # Select only numeric columns
  summarise(
    across(
      everything(),
      ~ ifelse(mean(.x, na.rm = TRUE) != 0, 
               sd(.x, na.rm = TRUE) / mean(.x, na.rm = TRUE), 
               NA),  # Compute CV safely
      .names = "{col}"
    )
  ) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(Variation = V1) %>%
  gt()
```

## Median Absolute Deviation MAD and MAD/median

The Median Absolute Deviation (MAD) is a robust measure of variability that quantifies the dispersion of a dataset. It is defined as the median of the absolute deviations from the median of the data:

$[
\text{MAD} = \text{median} \left( \left| x_i - \text{median}(x) \right| \right)
]$

where:

-   ( x_i ): Individual data points,
-   ( \text{median}(x) ): Median of the data.

The MAD/Median ratio is a normalized measure of dispersion, calculated as:

$[
\text{MAD/Median} = \frac{\text{MAD}}{\text{median}(x)}
]$

This ratio provides a scale-independent measure of variability, making it useful for comparing the dispersion of datasets with different units or scales. A higher MAD/Median ratio indicates greater relative variability.

```{r}
#| echo: true
#| message: false
#| warning: false

# MAD
delitos_data %>%
  select(where(is.numeric)) %>%
  summarise(across(everything(), ~ mad(.x, na.rm = TRUE))) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(MAD = V1) %>%
  gt()

# MAD/Median
delitos_data %>%
  select(where(is.numeric)) %>%
  summarise(across(everything(), ~ ifelse(
    median(.x, na.rm = TRUE) != 0,
    mad(.x, na.rm = TRUE) / median(.x, na.rm = TRUE),
    NA
  ))) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(`MAD/Median` = V1) %>%
  gt()

delitos_data %>%
  select(SEMOVIENTES, AUTOMOTORES, BANCOS, PIRATERIA) %>%
  summary()
```

## Covariance Matrix

The covariance matrix $( \Sigma )$ captures the pairwise covariances between variables in a dataset. For a dataset $( X )$ with $( n )$ observations and $( p )$ variables, the covariance matrix is defined as:

$[
\Sigma = \frac{1}{n-1} (X - \bar{X})^\top (X - \bar{X})
]$

where:

-   $( X )$ is the $( n \times p )$ data matrix.
-   $( \bar{X} )$ is the $( n \times p )$ matrix of column means.
-   $( \Sigma )$ is a $( p \times p )$ symmetric matrix.

```{r}
#| echo: true
#| message: false
#| warning: false
delitos_data %>%
  select(where(is.numeric)) %>%
  cov() %>%
  round(2) %>%
  knitr::kable(digits = 2, caption = "Covariance Matrix")
```

## Covariance Matrix of Log-Transformed Data

To handle skewed data or reduce the impact of outliers, we apply a log transformation to the data. Let $( Y = \log(X + 1) )$, where $( \log )$ is applied element-wise and $( 1 )$ is a matrix of ones to handle zeros. The log-transformed covariance matrix $( \Sigma_{\text{log}} )$ is:

$[
\Sigma_{\text{log}} = \frac{1}{n-1} (Y - \bar{Y})^\top (Y - \bar{Y})
]$

where:

-   $( Y )$ is the $( n \times p )$ log-transformed data matrix.
-   $( \bar{Y} )$ is the $( n \times p )$ matrix of column means of $( Y )$.

```{r}
#| echo: true
#| message: false
#| warning: false

delitos_data %>%
  select(where(is.numeric)) %>%
  mutate(across(everything(), ~ log(.x+1))) %>%  # Log-transform (+1 to handle zeros)
  cov() %>%
  round(2) %>%
  knitr::kable(digits = 2, caption = "Covariance Matrix (Log-Transformed)")
```

## Redundant Variables

```{r}
#| echo: true
#| message: false
#| warning: false

# Covariance matrix 
cm_delitos_data <- delitos_data %>%
  select(where(is.numeric)) %>%
  cov()

# Compute eigenvalues and eigenvectors
eigen_results <- cm_delitos_data %>% eigen()

# Extract eigenvalues and eigenvectors
eigenvalues <- eigen_results$values
eigenvectors <- eigen_results$vectors

# Display eigenvalues and eigenvectors
print(eigenvalues)
print(eigenvectors)

# The Smallest Eigenvalues
sort(eigenvalues, decreasing = FALSE)[1:5]

# The smallest eigenvalue is approximately zero
smallest_eigenvalue <- min(eigenvalues)
print(smallest_eigenvalue)

# Corresponding eigenvector
smallest_eigenvector <- eigenvectors[, which.min(eigenvalues)]
print(smallest_eigenvector)

# Normalize the eigenvector by dividing by the largest absolute value
normalized_eigenvector <- smallest_eigenvector / max(abs(smallest_eigenvector))
print(normalized_eigenvector)

# Sorted normalize the eigenvector
sort(abs(normalized_eigenvector), decreasing = T)

# Get numeric variable names (order matches eigenvector indices)
variable_names <- colnames(delitos_data %>% select(where(is.numeric)))

# Sort normalized eigenvector by absolute contribution (descending order)
sorted_contributions <- sort(abs(normalized_eigenvector), decreasing = TRUE)

# Get the indices of the top contributions
top_indices <- order(abs(normalized_eigenvector), decreasing = TRUE)[1:4]

# Get the names of the top variables
top_variable_names <- variable_names[top_indices]

# Print the top variable names
print(top_variable_names)

# Fit a regression model to confirm the relationship
model <- lm(BANCOS ~ PIRATERIA + SECUESTRO + TERRORISMO, 
            data = data.frame(cm_delitos_data))
summary(model)

# Variance Inflation Factors
vif(model)

par(mfrow = c(2, 2))
plot(model)
```

## Global Variability Metric

The effective variance and effective standard deviation are measures of the overall variability in the dataset. They are derived from the determinant of the covariance matrix, which captures the generalized variance of the data. For log-transformed data, these metrics are computed similarly but on the log-transformed covariance matrix.

The effective variance is defined as:

$[
\text{Effective Variance} = \det(\Sigma)^{\frac{1}{p}}
]$

where:

-   $( \Sigma )$ is the covariance matrix.
-   $( p )$ is the number of variables.

The effective standard deviation is given by:

$[
\text{Effective Standard Deviation} = \det(\Sigma)^{\frac{1}{2p}}
]$

For log-transformed data, the effective variance is computed as:

$[
\text{Log-Transformed Effective Variance} = \det(\log(\Sigma + 1))^{\frac{1}{p}}
]$

Similarly, the log-transformed effective standard deviation is:

$[
\text{Log-Transformed Effective Standard Deviation} = \det(\log(\Sigma + 1))^{\frac{1}{2p}}
]$

```{r}
#| echo: true
#| message: false
#| warning: false

cov_matrix <- delitos_data %>%
  select(-BANCOS) %>%
  select(where(is.numeric)) %>%  # Select numeric columns
  cov() 

# Effective Variance
det(cov_matrix)^(1/ncol(cov_matrix))

# Log-Transformed Effective Variance
det(log(cov_matrix + 1))^(1/ncol(cov_matrix))

# Effective Standard Deviation
det(cov_matrix)^(1/(ncol(cov_matrix) * 2))

# Log-Transformed Effective Standard Deviation
det(log(cov_matrix + 1))^(1/(ncol(cov_matrix) * 2))
```

## Linear Dependency and Precision Matrix

```{r}
#| echo: true
#| message: false
#| warning: false

# Compute precision matrix
S_inv <- solve(cov_matrix)

# Display precision matrix (should match example values)
cat("Precision Matrix (S⁻¹):\n")
print(S_inv, digits = 2)

# Extract first row components
first_row <- S_inv[1,]
diag_element <- S_inv[1,1]

# Calculate regression coefficients
beta_coefficients <- -first_row[-1]/diag_element
print(beta_coefficients, digits = 2)

# Calculate residual variance and standar error
residual_variance <- 1/diag_element
round(residual_variance, 2)
sqrt(round(residual_variance, 2))

# R^2
1 - (1/(cov_matrix[1,1] * S_inv[1,1]))

# Regression coefficients
delitos <- delitos_data %>%
  select(-BANCOS) %>%
  select(where(is.numeric))

model <- lm(SEXUALES ~ ., data = data.frame(delitos))
summary(model)
```

# Graphical analysis {.unnumbered}

## Distribution of Crime Data

```{r}
#| echo: true
#| message: false
#| warning: false

# Load necessary library
library(ggplot2)
library(tidyr)

# Transform the data to a long format for ggplot
delitos_long <- delitos_data %>%
  select(where(is.numeric)) %>%  # Select only numeric columns
  pivot_longer(cols = everything(), names_to = "Crime Type", values_to = "Value")

# Create faceted histograms
ggplot(delitos_long, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
  facet_wrap(~ `Crime Type`, scales = "free") +  # Facet by crime type
  theme_minimal() +
  labs(
    title = "Distributions of Crime Data",
    x = "Value",
    y = "Frequency"
  ) +
  theme(
    axis.text.x = element_text(size = 5)  # Reduce the font size of X-axis text
  )
```

## Log-Transformed Crime Data Distributions

```{r}
#| echo: true
#| message: false
#| warning: false

# Transform the data to long format and apply log transformation
delitos_long_log <- delitos_data %>%
  select(where(is.numeric)) %>%
  mutate(across(everything(), ~ log(.x), .names = "{col}")) %>%  # Log transform (log(x + 1) to avoid log(0))
  pivot_longer(cols = everything(), names_to = "Crime Type", values_to = "Log Value")

# Create faceted histograms for log-transformed values
ggplot(delitos_long_log, aes(x = `Log Value`)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
  facet_wrap(~ `Crime Type`, scales = "free") +  # Facet by crime type
  theme_minimal() +
  labs(
    title = "Log-Transformed Distributions of Crime Data",
    x = "Log Value",
    y = "Frequency"
  ) +
  theme(
    axis.text.x = element_text(size = 3)  # Reduce the font size of X-axis text
  )
```

```{r}
#| echo: true
#| message: false
#| warning: false

# Transform the data to long format and apply log transformation
delitos_long_log <- delitos_data %>%
  select(where(is.numeric)) %>%
  mutate(across(everything(), ~ log(.x), .names = "{col}")) %>%  # Log transform (log(x + 1) to avoid log(0))
  pivot_longer(cols = everything(), names_to = "Crime Type", values_to = "Log Value")

# Create faceted histograms for log-transformed values
ggplot(delitos_long_log, aes(x = `Log Value`)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
  facet_wrap(~ `Crime Type`, scales = "free") +  # Facet by crime type
  theme_minimal() +
  labs(
    title = "Log-Transformed Distributions of Crime Data",
    x = "Log Value",
    y = "Frequency"
  ) +
  theme(
    axis.text.x = element_text(size = 3)  # Reduce the font size of X-axis text
  )
```

# Hands-on Data Analysis {.unnumbered}

-   Divide stations by regions of analysis to explain the spatial correlations.
-   Replicate exploratory analysis in the selected regions.
-   Load your work by GitHub pull request.
-   Define the issue for decision-making.